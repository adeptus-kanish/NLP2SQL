# üß† NLP2SQL

Convert natural language questions into SQL using a local LLM (Mistral 7B) and query a CSV-based employee dataset. This is Phase 1 of a multi-phase project toward an agent-based RAG system.

---

## üìÅ Directory Structure

```bash
.  
‚îú‚îÄ‚îÄ cli_chat.py # Interactive CLI interface  
‚îú‚îÄ‚îÄ main.py # Entry point for CLI  
‚îú‚îÄ‚îÄ llm/ # Local model loading, tokenizer, prompt handling  
‚îÇ ‚îú‚îÄ‚îÄ loader.py  
‚îÇ ‚îú‚îÄ‚îÄ singleton.py  
‚îÇ ‚îú‚îÄ‚îÄ model/ # Mistral .gguf file  
‚îÇ ‚îú‚îÄ‚îÄ tokenizer/  
‚îÇ ‚îî‚îÄ‚îÄ prompts/ # Custom prompt templates (coming soon)  
‚îú‚îÄ‚îÄ data/  
‚îÇ ‚îî‚îÄ‚îÄ simple_employee.csv # Sample data for SQL queries  
‚îú‚îÄ‚îÄ schema/  
‚îÇ ‚îî‚îÄ‚îÄ employee_schema.json # JSON schema of the dataset  
‚îú‚îÄ‚îÄ utils/  
‚îÇ ‚îî‚îÄ‚îÄ chroma_utils.py # ChromaDB indexing and retrieval (coming soon)  
‚îú‚îÄ‚îÄ config.yaml # Configuration for future agents  
‚îî‚îÄ‚îÄ README.md # This file
````

---

## üöÄ Quickstart

### 1. Clone & Setup Environment

```bash
git clone https://github.com/adeptus-kanish/NLP2SQL
cd NLP2SQL
python -m venv ragenv
source ragenv/bin/activate
pip install -r requirements.txt
````

---

### 2. Place Model

Download a quantized `Mistral-7B-Instruct-v0.3` model (`.gguf`) and place it in:

```
llm/model/mistral-7b-instruct-v0.3-q4_k_m.gguf
```

---

### 3. Run the CLI

```bash
python main.py
```

You should see:

```
Welcome to the NLP2SQL CLI. Type 'exit' to quit.
üí¨ You:
```

---

## üí¨ Sample Questions to Ask

- `List all employees from IT`
    
- `Who has the highest salary?`
    
- `Employees who joined after 2015`
    
- `Average salary of Sales department`
    

> The model responds with SQL, which will be used in future phases to run actual queries.

---

## üìå Real Use Case Examples

| User Query                                       | Action Type           |
| ------------------------------------------------ | --------------------- |
| ‚ÄúList all employees who joined before 2015‚Äù      | ‚Üí SQL                 |
| ‚ÄúWhat is the meaning of join\_date?‚Äù             | ‚Üí Vector search (RAG) |
| ‚ÄúWhich columns are there in this dataset?‚Äù       | ‚Üí Vector search (RAG) |
| ‚ÄúHow many people joined in HR after 2010?‚Äù       | ‚Üí SQL                 |
| ‚ÄúWhat‚Äôs the definition of bonus in this schema?‚Äù | ‚Üí RAG                 |

---

## Phase 1

| Area                    | Status                          | Notes                                              |
| ----------------------- | ------------------------------- | -------------------------------------------------- |
| **Model Directory**     | ‚úÖ `llm/model/`                  | `mistral-7b-instruct-v0.3-q4_k_m.gguf` is in place |
| **Local Loading Logic** | ‚úÖ `loader.py` + `singleton.py`  | Loads the GGUF model via llama.cpp (with Metal)    |
| **Prompt Execution**    | ‚úÖ Works end-to-end              | Generates SQL or freeform answers                  |
| **Data**                | ‚úÖ `data/simple_employee.csv`    | Enough for mock evaluation                         |
| **Schema**              | ‚úÖ `schema/employee_schema.json` | Good for later RAG (Phase 2)                       |
| **Utilities**           | ‚úÖ `utils/chroma_utils.py`       | Placeholder for Chroma DB ops                      |
| **Virtual Env**         | ‚úÖ `ragenv/`                     | Fully isolated environment                         |

---

## Phase 2

| Task                           | File                           | Description                                                                                |
| ------------------------------ | ------------------------------ | ------------------------------------------------------------------------------------------ |
| **1. Add SQLite DB**           | `data/employees.db` or similar | Create and populate a small SQLite database with dummy employee data (if not already done) |
| **2. Create SQL executor**     | `executor.py`                  | A module to execute SQL queries safely and return results or error messages                |
| **3. Connect executor to CLI** | `cli_chat.py`                  | After LLM outputs SQL, run it and show results                                             |
| **4. Handle exceptions**       | `executor.py` + `cli_chat.py`  | Show friendly error messages if SQL is invalid                                             |
| **5. Improve UX**              | `cli_chat.py`                  | Add a loop to ask for more queries until user exits                                        |

---

## Phase 3

| Step | Task                                             | Description                                       |
| ---- | ------------------------------------------------ | ------------------------------------------------- |
| 1    | Setup & store schema/query knowledge in ChromaDB | Index schema + sample queries                     |
| 2    | Add semantic retriever (`chroma_utils.py`)       | Flexible chunk search based on user query         |
| 3    | Decide route: LLM SQL vs RAG                     | Add router logic to use LLM or fallback to Chroma |
| 4    | Print clean result in CLI                        | RAG fallback answer if no SQL                     |

---

## üë®‚Äçüîß Author

Built by [Kanish](https://github.com/kanish-h-h) ‚Äî powered by Mistral and ü§ó Transformers.
